Machine-Level Representation of Programs

Computers execute machine code, 
sequences of bytes encoding the low-level operations that manipulate data, 
manage memory, 
read and write data on storage devices, 
and communicate over networks. 

A compiler generates machine code through a series of stages, 
based on the rules of the programming language, 

the instruction set of the target machine, 
and the conventions followed by the operating system. 

The gcc C compiler generates its output in the form of assembly code,
a textual representation of the machine code giving the individual instructions in
the program. 

gcc then invokes both an assembler and a linker to 
generate the executable machine code from the assembly code. 

In this chapter, we will take a close look at machine code and 
its human-readable representation as assembly code.

When programming in a high-level language such as C, 
and even more so in Java, 
we are shielded from the detailed, 
machine-level implementation of our program. 

In contrast, when writing programs in assembly code (as was done in the
early days of computing) a programmer must specify the low-level instructions the
program uses to carry out a computation. 

Most of the time, it is much more productive and reliable to work at the higher level of abstraction provided by a high-level
language. 

The type checking provided by a compiler helps detect many program errors 
and makes sure we reference and manipulate data in consistent ways. 

With modern, optimizing compilers, 
the generated code is usually at least as efficient as what a skilled, 

assembly-language programmer would write by hand. 

Best of all, a program written in a high-level language can be compiled 
and executed on a number of different machines, 
whereas assembly code is highly machine specific.

So why should we spend our time learning machine code? 

Even though compilers do most of the work in generating assembly code, 
being able to read and understand it is an important skill for serious programmers. 

By invoking the compiler with appropriate command-line parameters, 

the compiler will generate a file showing its output in assembly-code form. 

By reading this code, we can understand the optimization capabilities of the compiler 
and analyze the underlying inefficiencies in the code. 

As we will experience in Chapter 5, programmers seeking to maximize the performance of a critical section of code often try different
variations of the source code, 
each time compiling and examining the generated
assembly code to get a sense of how efficiently the program will run. 

Furthermore,
there are times when the layer of abstraction provided by a high-level language
hides information about the run-time behavior of a program that we need to understand. 

For example, when writing concurrent programs using a thread package,
as covered in Chapter 12, 
it is important to know what region of memory is used to hold the different program variables. 

This information is visible at the assembly-code level. 

As another example, many of the ways programs can be attacked,
allowing worms and viruses to infest a system, 
involve nuances of the way programs store their run-time control information. 

Many attacks involve exploiting weaknesses in system programs to overwrite information 
and thereby take control of the system. 

Understanding how these vulnerabilities arise and how to guard
against them requires a knowledge of the machine-level representation of programs. 

The need for programmers to learn assembly code has shifted over the years 
from one of being able to write programs directly in assembly to one of
being able to read and understand the code generated by compilers.

In this chapter, we will learn the details of two particular assembly languages
and see how C programs get compiled into these forms of machine code. 

Reading the assembly code generated by a compiler involves a different set of skills than
writing assembly code by hand. 

We must understand the transformations typical compilers make in converting 
the constructs of C into machine code. 

Relative to the computations expressed in the C code, 
optimizing compilers can rearrange execution order, 
eliminate unneeded computations, replace slow operations with faster ones, 
and even change recursive computations into iterative ones. 

Understanding the relation between source code and the generated assembly can often be a challenge—

it’s much like putting together a puzzle having a slightly different design than the picture on the box. 

It is a form of reverse engineering—trying to understand the process by which a system was created by studying the system
and working backward. 

In this case, the system is a machine-generated assembly-language program, 
rather than something designed by a human. 

This simplifies the task of reverse engineering, 
because the generated code follows fairly regular patterns, 
and we can run experiments, 
having the compiler generate code for many different programs. 

In our presentation, we give many examples and provide a number of exercises illustrating different aspects of 
assembly language and compilers. 

This is a subject where mastering the details is a prerequisite to understanding the deeper and more fundamental concepts. 

Those who say “I understand the general principles, I don’t want to bother learning the details” are deluding themselves. 

It is critical for you to spend time studying the examples, 
working through the exercises, and checking your solutions with those provided.

Our presentation is based on two related machine languages: Intel IA32, 
the dominant language of most computers today, 
and x86-64, 

its extension to run on 64-bit machines. 

Our focus starts with IA32. 

Intel processors have grown from primitive 16-bit processors in 1978 
to the mainstream machines for today’s desktop, laptop, and server computers. 

The architecture has grown correspondingly,
with new features added and with the 16-bit architecture transformed to become IA32, 

supporting 32-bit data and addresses. 

The result is a rather peculiar design
with features that make sense only when viewed from a historical perspective. It
is also laden with features providing backward compatibility that are not used by
modern compilers and operating systems. We will focus on the subset of the fea-
tures used by gcc and Linux. This allows us to avoid much of the complexity and
arcane features of IA32.


Our technical presentation starts with a quick tour 
to show the relation between C, assembly code, and machine code. 

We then proceed to the details of IA32, 
starting with the representation and manipulation of data and the implementation of control. 

We see how control constructs in C, such as if, while, and switch statements, are implemented. 

We then cover the implementation of procedures, 

including how the program maintains a run-time stack to support 
the passing of data and control between procedures, 

as well as storage for local variables. 

Next, we consider how data structures such as arrays, structures, 
and unions are implemented at the machine level. 

With this background in machine-level programming, 
we can examine the problems of out of bounds memory references and
the vulnerability of systems to buffer overflow attacks. 

We finish this part of the presentation with some tips on 
using the gdb debugger for examining the run-time
behavior of a machine-level program.

As we will discuss, the extension of IA32 to 64 bits, termed x86-64, 
was originally developed by Advanced Micro Devices (AMD), 
Intel’s biggest competitor.

Whereas a 32-bit machine can only make use of around 4 gigabytes (2^32 bytes) of random-access memory, 

current 64-bit machines can use up to 256 terabytes (2^48 bytes). 

The computer industry is currently in the midst of a transition from 32-bit to 64-bit machines. 

Most of the microprocessors in recent server and desktop machines, 
as well as in many laptops, support either 32-bit or 64-bit operation.

However, most of the operating systems running on these machines support only 32-bit applications, 
and so the capabilities of the hardware are not fully utilized.
As memory prices drop, and the desire to perform computations involving very
large data sets increases, 64-bit machines and applications will become common-
place. It is therefore appropriate to take a close look at x86-64. We will see that in
making the transition from 32 to 64 bits, the engineers at AMD also incorporated
features that make the machines better targets for optimizing compilers and that
improve system performance.
































