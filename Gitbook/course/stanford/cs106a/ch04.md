计算机历史
So before we dive into that, I want you to get a little notion of the history of computing. Okay, and we’ll go over this very briefly, but computing actually has a very long history and it dates back about 4,000 years, okay. So roughly 4,000 years ago, the first one we think of is computing device was made available, but then you want to know what that was? Advocates.
The first computing device allowed you to do some fairly rudimentary arithmetic on it, but it actually was something that allowed people to compute a lot faster than they could by keeping track of stuff in their head. The real sort of advances in what we think of as computing actually came around in the 1800’s. That was someone by the name Charles Babbage. Anyone heard of Charles Babbage, by the way, a just quick show of hands. We can just call him Chuck.
He was actually a very well learned and well-known person in his day. He had the Lucasian chair, which is the same chair at university that actually Steven Hawking has now that Sir Isaac Newton had at one point. So, pretty smart dude, and he came up with this notion of something called the Difference Engine, which is a way of being able to automatically.
And in those days, right, they didn’t have silicon. Well, they had silicon, but it was in the form of sand, and they didn’t have computers in the way we think of computers, so he wanted to build a mechanical device like with real, actual, you know, mechanics and pieces of woods that’s been around, and in his time, he thought is would be powered by steam, right, because this is sort of pre thinking about electronic computers that would do automated calculation.
And he designed something called the Difference Engine and then he designed something called the Analytic Engine, which was supposed to be even more powerful, and interestingly enough neither one of them was actually built during his lifetime.
It turns out that years later, the difference engine was actually built from a bunch of wheels and rotors and stuff and now the things, you know, that he would have liked to think of as the Analytic Engine is actually these puppies. It actually turned in the 1800’s; he had a lot of the same ideas that are in our laptops or in our computers today strangely enough.
But, so that’s kind of where our computing we think of the notion of computing is really coming from, and it turns out the first programmer was a woman by the name of Ada Byron, actually Augusta Ada Byron and if the Byron looks familiar, too, is because she was the daughter of Lord Byron, the English poet and she was actually very taken by the designs of Charles Babbage’s machines and actually wrote programs.
The machines were actually written to have sort of cards, sort of like the Jacard Loom if you’ve ever heard of the Jacard Loom, if you haven’t, it’s not important, that actually would have programs for the kind of computations that it would do and see, she actually devised various programs for Charles Babbage’s Analytical Engine.
And you might be sitting there and you might think, “But Marion, you’ve just told me that the Analytical Engine was never actually built in their lifetime, so what’s she doing writing programs for a computer that doesn’t exist,” and that’s actually something that happens today. People write programs for computers that don’t exist and you might wonder why. That’s not a very good use of your time.
Well, guess what, if the next generation of computers is going to come out while it’s still being designed before it’s actually manufactured and built, someone needs to figure out what kind of programs you want to run on that machine, so there are actually programmers today who actually write programs for machines that aren’t built. And they simulate them by hand and they go through and try to figure out what they would do and it’s perfectly reasonably thing to do.
But as a result, Ada Byron is in some sense, the first programmer, right, because she was actually writing programs for in some sense, a general purpose computational device over 100 years ago, which is kind of an astounding thing if you think about it.
Now it turns out computing actually got its first real, you know, what we think more closely of as computing devices in the 1930’s. I should put 19 here just so we’re clear – 1930’s and 40’s, they were sort of the first prototypes of electro mechanical computers that were actually built at the time.
There was a computer that was built at Iowa State by Atanasoff and Berry. Their names are actually not – well, they’re important if you’re a lawyer because it turned out there were actually lawsuits at the time by who patented it in the computer, but we won’t talk about that.
Started sort of in this era with sort of prototypes and then the one machine you may have actually heard of by 1946, there was a machine called ENIAC, which was actually built by Elkhart and McCauley at U Penn which was standard for, if I can remember this, for electronic, numerical integrator and calculator.
And basically with one of these big things, you know, sat in a big warehouse, a few tons, but it actually did computational, you know, the kinds of things we would think of as computation and it was really in some sense the first large-scale electro magnetic computer that we think about, again, you know, modular lawyers.
And then really it was in 1971, that the first microprocessor came around, so we come to a fairly modern time, right, so it’s not all that long ago, like we’re talking 36 years, the microprocessors have actually been around.
And so the first microprocessor and one of the folks who was on that team who built the first microprocessor, Ted Hawk, is actually Stanford alum, interestingly enough, it was built at Intel. The Intel 4004 was the name of that microprocessor and at the time, no one actually thought that having a single chip microprocessor was going to be that interesting.
He sort of designed this thing and it was originally going to be going over for use in some machine in Japan and they were just going to give them some design and the patent for it and the folks were getting like, “Well, we’re not really interested. Yeah, sure it’s an interesting chip, but we’re not really interested in, you know, owning the patent. We don’t think there’s anything interesting necessarily here,” and so they kind of ran with it and the rest is history, as they say.
And as you can imagine in the last 35 years or so, there’s just been an astounding amount of development in computing and computer science, right, so really this field is like hundreds or thousands years old, if you think about it, but really it’s kind of in your lifetime that all the interesting stuff is really happening, which is also why it’s exciting to be alive now and be a computer scientist because you’re in this acceleration phase, right.
Think about what’s going to happen, like all the stuff that happened in the last 36 years, you’re gonna be around to see the next 36 years and you’re gonna be doing the next 36 years, so that’s just an exciting thing that I think about.


计算机是工程还是科学
Well, if you think about computer engineering, part of it is just kind of a definition that a particular school wants to have for it, right. And some places actually – our computer science program is in the School of Engineering. Some places it’s actually in humanities and sciences. It depends on the view and it also depends on the particular program that’s there, so some places may actually –there’s no way I'm gonna hit you in the back – may just be training programmers, right.
And the science of computing is not what they’re actually interested in, and so it depends on the program, but we really think of the computer science as a science, and it’s an interesting philosophical argument. We can certainly talk about it more in class, but I just wanted to kind of push on.
All right, so with that said, that’s kind of the quick tour of a couple hundred years of computer science history, it turns out when we get into modern day, when we actually think about computing, what does a computer really understand and what are the programs that you write on it and how do they turn into something that the computer understands.
So it turns out that the computer actually understands zeros and ones. And so we like to think of that as binary, right. Binary is just a number system that only has zeros and ones. It’s base two numbers basically and that’s all that computers understands, zeros and ones on and off.

JVM为了使代码可以在不同的计算机上运行
In Java’s world, part of what’s going on is actually run on a virtual machine. What does that mean; it means you write some source file in Java, okay, that goes through some compiler and what comes out of it is not an object file, but something called a class file.

It takes all of the high-level stuff that you write in Java or in this case, you could say Karel because it’s the same thing, and turns it into some set of numerical instructions that are not yet ones and zeros that the computer understands but are some intermediate language, which is just a numerical language.

And sometimes you refer to this as Java bite code, but the name is actually not important. It’s some intermediate language, and guess what, there’s other classes just like there were before that contain instructions in this intermediate language and those all get linked together in some big file that we call a jar archive which just stands for jar just means Java archive, strangely enough.

So it’s actually redundant to say jar archive. People just do, but it means Java archive. All right, and then this whole thing is now instead of this intermediate language, it’s something that neither the human really understands – okay, there’s a few humans in the world that might and they’re a little weird and we won’t talk about them. Most people don’t understand this and the computer doesn’t understand it directly either.


an object-oriented language

a class is just basically an encapsulation of some behavior that the program does
also have some data, so it’s behavior and data

从人 灵长类 哺乳动物 动物 解释 面向对象(动物，哺乳动物，灵长类动物，人类和猴子)
So we have humans, okay. Humans are just a class, okay, and all humans are primates. Primates are a super class of humans, because all humans are primates, all primates are mammals, and all mammals are animals. So there’s actually a much deeper and richer hierarchy say in biology then there is with Karel and when you actually see Java, Java will eventually have sort of a richer hierarchy associated with it.

Now, the interesting thing about this, this is not only humans primates, but monkeys are also primates, okay. Humans are not monkeys and monkeys are not humans, so there’s not a strict – you might say, “Well, we don’t know about that.” But there isn’t a strict relationship between humans and monkeys other than they’re both primates, and that makes them mammals, and that makes them animals, and the whole notion of having this hierarchy of classes, right, so you could think of being animal as just a class and it has some behaviors. What does it mean to be an animal?

Well, to be an animal means you digest food, right, it’s not something like something like Inaudible actually, and one of the technical terms – I'm not a biologist, but just what my friends tell me, embryos pass through the blastula stage. Any biologists in here? Is that right?

the instance of a class
an instance of human

you create which are objects and what an object is to differentiate it from a class

子类的继承
that’s the whole point. This is the class hierarchy. These are all classes and things that are sub classes of some other class means they inherit all of the behaviors of the things above them, all the way up the chain plus they may have their own additional behaviors and that’s the key concept here of organizing things in classes and you’ll see that in more specific instance when you get into Java.
 
ACM 计算机组织
the Association for Computing Machinery


